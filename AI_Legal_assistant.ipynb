{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yeNkHY0XCLFF",
        "outputId": "8da52e31-9437-4708-e4cb-06dfb12eee54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU  langchain langchain_core huggingface_hub kagglehub chromadb langchain-groq\n",
        "!pip install -qU  langchain langchain_community langchain_huggingface langchain-chroma taipy\n",
        "!pip install -qU  gradio pypdf tiktoken sentence_transformers langgraph pandas matplotlib jq\n",
        "!pip install -qU \"langchain-chroma>=0.1.2\" pinecone langchain_pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get('pinecone_key')\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('langchai_api_key')\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = 'Law GPT'\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get('groq_api_key')\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HF_TOKEN')\n",
        "\n",
        "#Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "d2hfI8M_16F8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akl1mI5ztuwS"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ayeshajadoon/pakistan-law-data\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Look at the downloaded files\n",
        "files = os.listdir(path)\n",
        "print(\"Files in dataset directory:\", files)\n",
        "\n",
        "\n",
        "file_paths = [os.path.join(path, file) for file in files]\n",
        "print(\"Full paths to dataset files:\", file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Assuming file_paths is already defined and contains the path to the JSON file\n",
        "if file_paths:\n",
        "    json_file_path = file_paths[0] # Assuming the first file is the json\n",
        "\n",
        "    # Load the JSON file into a pandas DataFrame\n",
        "    df = pd.read_json(json_file_path)\n",
        "\n",
        "    # Display the head of the DataFrame\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"No files found in file_paths.\")\n"
      ],
      "metadata": {
        "id": "uhoUDXX7XL75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4vREXh7rfLo"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import JSONLoader, CSVLoader, PyPDFLoader\n",
        "\n",
        "def data_loader(file_paths):\n",
        "    all_documents = []\n",
        "    for file_path in file_paths:\n",
        "        if file_path.endswith('.csv'):\n",
        "            loader = CSVLoader(file_path)\n",
        "        elif file_path.endswith('.json'):\n",
        "            loader = JSONLoader(file_path, jq_schema='.', text_content=False)\n",
        "        elif file_path.endswith('.pdf'):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Unsupported file type: {file_path}\")\n",
        "            continue\n",
        "        all_documents.extend(loader.load())\n",
        "    return all_documents\n",
        "\n",
        "data_loader(file_paths[:30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ5cA7m0rccd"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "\n",
        "# Data Splitter\n",
        "chunk_size = 500\n",
        "chunk_overlap = 50\n",
        "\n",
        "def text_splitter(data):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=chunk_size,\n",
        "      chunk_overlap=chunk_overlap\n",
        "    )\n",
        "  split_text = text_splitter.split_documents(data)\n",
        "  return split_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxXZpl4vraCA"
      },
      "outputs": [],
      "source": [
        "## Vector db\n",
        "from pinecone import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from pinecone import Pinecone\n",
        "from uuid import uuid4\n",
        "\n",
        "\n",
        "pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)\n",
        "\n",
        "def vector_database(chunks):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    index= pc.Index(\"lawindex\")\n",
        "    vector_store = PineconeVectorStore(index=index, embedding=embeddings)\n",
        "    # vector_store.add_documents(documents=chunks)\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEz9LIgWrWNr"
      },
      "outputs": [],
      "source": [
        "## Retriever\n",
        "def Retriever(file):\n",
        "    splits = data_loader(file)\n",
        "    chunks = text_splitter(splits)\n",
        "    vectordb = vector_database(chunks) # vector_database now returns a Chroma vector store\n",
        "    retriever = retriever = vectordb.as_retriever(\n",
        "        search_type=\"similarity_score_threshold\",\n",
        "        search_kwargs={\"k\": 5, \"score_threshold\": 0.4},\n",
        "    )\n",
        "    return retriever\n",
        "\n",
        "retriever = Retriever(file_paths)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict , Dict\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  keys: Dict[str , any]"
      ],
      "metadata": {
        "id": "ykB68XDN_9qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import template\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser , PydanticOutputParser\n",
        "from langchain_core.pydantic_v1 import BaseModel , Field\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "from langchain_core.tools import tool\n",
        "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"deepseek-r1-distill-llama-70b\", model_provider=\"groq\")\n",
        "\n",
        "#nodes\n",
        "\n",
        "def retrieve(state):\n",
        "  print(\"----RETRIEVE----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = retriever.get_relevant_documents(question)\n",
        "  return {\"keys\": {\"documents\": documents , \"question\": question}}\n",
        "\n",
        "\n",
        "\n",
        "# Attach File\n",
        "\n",
        "def attach_file(state,user_file):\n",
        "    print(\"----File Attachment----\")\n",
        "    state_dict = state[\"keys\"]\n",
        "    question = state_dict[\"question\"]\n",
        "    documents = state_dict[\"documents\"]\n",
        "    file_content = data_loader(user_file)\n",
        "    file_text = \" \".join([doc.page_content for doc in file_content])\n",
        "    documents.append(Document(page_content=file_text))\n",
        "    return {\"keys\": {\"documents\": documents , \"question\": question}}\n",
        "\n",
        "def generation(state):\n",
        "  print(\"----GENERATION----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "\n",
        "  #prompt\n",
        "  prompt = ChatPromptTemplate.from_template(\"\"\"You are LAW GPT, a legal research assistant with a distinctive scholarly persona. Adopt these characteristics:\n",
        "\n",
        "      1. **Persona**: A 19th-century legal scholar reincarnated as an AI. Use subtle Victorian-era formalism mixed with modern clarity. Signature phrases: \"In the matter of...\", \"Wherefore we observe...\", \"The jurisprudence suggests...\"\n",
        "\n",
        "      2. **Information Integration**:\n",
        "        - Always ground responses in retrieved legal documents. Add refrences from Pakistan Law acts\n",
        "        - Cite sources using Bluebook-style abbreviations (e.g., 347 U.S. 483) as much  possible\n",
        "        - When referencing statutes, include: <Jurisdiction> <Code> ¬ß <Section> (<Year>)\n",
        "\n",
        "      3. **Response Structure**:\n",
        "        ```legal\n",
        "        [Emblematic Header]\n",
        "        (e.g., \"IN THE MATTER OF [USER'S QUERY BRIEF]\")\n",
        "\n",
        "        [Context Bridge]\n",
        "        Connect query to historical legal evolution (\"This question echoes the doctrinal shift in...\")\n",
        "\n",
        "        [Retrieved Authority]\n",
        "        Present 2-3 most relevant provisions with pinpoint citations:\n",
        "        ‚Ä¢ <Source 1> [Relevance explanation]\n",
        "        ‚Ä¢ <Source 2> [Contrasting interpretation]\n",
        "\n",
        "        [Modern Application]\n",
        "        Apply principles to user's specific circumstances with hypothetical:\n",
        "        \"Consider a scenario where [user's situation]... Under [Cited Authority], the outcome would likely turn on...\"\n",
        "\n",
        "        [Scholarly Caveat]\n",
        "        \"We note jurisprudential tension in...\" + \"Consult local counsel regarding...\n",
        "      User Query:\n",
        "      {question}\n",
        "\n",
        "      Retrieved Legal Documents:\n",
        "      {context}\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "  rag_chain = prompt | llm | StrOutputParser()\n",
        "  #generation\n",
        "  generation = rag_chain.invoke({\"context\": documents , \"question\": question})\n",
        "  return {\"keys\": {\"generation\": generation , \"documents\" : documents , \"question\": question}}\n",
        "\n",
        "def grade_documents(state):\n",
        "  print(\"----Check Relevance----\")\n",
        "  state_dict= state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "\n",
        "  class grade(BaseModel):\n",
        "    \"\"\" check the relevance documents\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=(\"Check binary score 'yes' or 'no' \")\n",
        "    )\n",
        "\n",
        "  #prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template= \"\"\"You are a grader accessing relevance of retrieved documents to the user question.\\n\n",
        "      Here is the retrieved documnets:\\n{context}.\\n\n",
        "      Here is the user Question:{question}.\\n\n",
        "      If the documents contain keyword(s) or semantic meaning relative to the user question ,grade them as relevant.\\n\n",
        "      Give a relevance score 'yes' or 'no' score for all documents to indicate that weather all documents are relevant.\\n\n",
        "      your response should be in json format:\"\"\",\n",
        "      input_variables=[\"context\" , \"question\"]\n",
        "  )\n",
        "\n",
        "  chain= prompt | llm.with_structured_output(grade,include_raw=True)\n",
        "\n",
        "  search = \"no\"\n",
        "  filtered_docs=[]\n",
        "  for d in documents:\n",
        "    score = chain.invoke({\"context\": d.page_content , \"question\": question})\n",
        "    if isinstance(score, tuple) and len(score) > 0 and hasattr(score[0], 'binary_score'):\n",
        "      if score[0].binary_score == \"yes\":\n",
        "            filtered_docs.append(d)\n",
        "      else:\n",
        "            print(\"----Documents are not relevant\")\n",
        "            search = \"yes\"\n",
        "    else:\n",
        "        # If structured output parsing failed, print message and potentially log the issue\n",
        "        print(\"----Could not parse relevance score, skipping document\")\n",
        "        # Optionally, you can log the raw 'score' value for debugging\n",
        "  return {\"keys\": {\"documents\": filtered_docs , \"question\": question , \"search\": search}}"
      ],
      "metadata": {
        "id": "ZblPm-lWAANg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain.schema import Document\n",
        "\n",
        "def translate_query(state):\n",
        "  print(\"----Translate Query----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "\n",
        "  #prompt\n",
        "  prompt = PromptTemplate(\n",
        "      template=\"\"\" You are generating question that is well optimized for retrieval.\\n\n",
        "      Look at the input and try to reason about underlying semantic intent / meanings.\\n\n",
        "      Here is the initial question:\n",
        "      \\n-------\\n\n",
        "      {question}\n",
        "      \\n-------\\n\n",
        "      Formulate an improved question:\"\"\",\n",
        "      input_variables=[\"question\"]\n",
        "  )\n",
        "\n",
        "  #chain\n",
        "  chain= prompt | llm | StrOutputParser()\n",
        "  new_question = chain.invoke({\"question\": question})\n",
        "  return {\"keys\": {\"question\": new_question , \"documents\": documents}}\n",
        "\n",
        "def web_search(state):\n",
        "  print(\"----Web Search----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "\n",
        "  tavily = TavilySearchResults(max_results=1)\n",
        "  tavily_search= tavily.invoke(question)\n",
        "  web_results = \"\\n\".join([d[\"content\"] for d in tavily_search])\n",
        "  web_results = Document(page_content=web_results)\n",
        "  documents.append(web_results)\n",
        "  return {\"keys\": {\"documents\": documents , \"question\": question}}\n",
        "\n",
        "def decide(state):\n",
        "  print(\"----Decide----\")\n",
        "  state_dict = state[\"keys\"]\n",
        "  question = state_dict[\"question\"]\n",
        "  documents = state_dict[\"documents\"]\n",
        "  search = state_dict[\"search\"]\n",
        "\n",
        "  if search == \"yes\":\n",
        "    print(\"----DECISION: Translate Query and Search the Web\")\n",
        "    return \"translate\"\n",
        "  else:\n",
        "    print(\"----DECISION: Generation\")\n",
        "    return \"generation\"\n",
        "\n"
      ],
      "metadata": {
        "id": "530Pe63-AH4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph ,END\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "\n",
        "graph.add_node(\"retrieve\" , retrieve) #retrieve\n",
        "graph.add_node(\"generation\", generation) #generation\n",
        "graph.add_node(\"grade_documents\", grade_documents) #grade_documents\n",
        "graph.add_node(\"translate_query\", translate_query) #translate\n",
        "graph.add_node(\"web_search\", web_search) #web_search\n",
        "\n",
        "\n",
        "graph.set_entry_point(\"retrieve\")\n",
        "graph.add_edge(\"retrieve\" , \"grade_documents\")\n",
        "graph.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide,\n",
        "    {\n",
        "        \"translate\": \"translate_query\",\n",
        "        \"generation\": \"generation\"\n",
        "    }\n",
        ")\n",
        "graph.add_edge(\"translate_query\", \"web_search\" )\n",
        "graph.add_edge(\"web_search\",\"generation\")\n",
        "graph.add_edge(\"generation\", END)\n",
        "\n",
        "app = graph.compile()"
      ],
      "metadata": {
        "id": "5-2YpFZbAP-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = \"what is pakistan law about public tax?\"\n",
        "for output in app.stream({\"keys\": {\"question\":inputs}},{\"recursion_limit\":150}):\n",
        "  for key, value in output.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "HSA0iOigASJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def law_gpt_interface(question: str, file):\n",
        "    \"\"\"\n",
        "    Processes a user query and an optional file attachment using the LangGraph RAG chain.\n",
        "\n",
        "    Args:\n",
        "        question: The user's text query.\n",
        "        file: The uploaded file object from Gradio (can be None).\n",
        "\n",
        "    Returns:\n",
        "        The generated response from the RAG chain.\n",
        "    \"\"\"\n",
        "    inputs = {\"keys\": {\"question\": question}}\n",
        "\n",
        "    # If a file is attached, process it and add its content to the state\n",
        "    if file is not None:\n",
        "        attach_file()\n",
        "\n",
        "    response = \"\"\n",
        "    # Stream the response from the LangGraph app\n",
        "    for output in app.stream(inputs, {\"recursion_limit\": 150}):\n",
        "        for key, value in output.items():\n",
        "            if key == \"generation\":\n",
        "                # Extract the generated text from the output\n",
        "                if \"keys\" in value and \"generation\" in value[\"keys\"]:\n",
        "                    response += value[\"keys\"][\"generation\"]\n",
        "    return response"
      ],
      "metadata": {
        "id": "GlYAw7AjH4Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(css=\"\"\"\n",
        "    body { font-family: sans-serif; }\n",
        "    .gradio-container { max-width: 2000px; margin: auto; padding: 20px; background-color: #f0f0f0; border-radius: 10px; }\n",
        "    h1 { text-align: center; color: #0056b3; }\n",
        "    .input-box { border: 1px solid #ccc; padding: 10px; border-radius: 5px; background-color: #fff; }\n",
        "    .output-box { border: 1px solid #ccc; padding: 10px; border-radius: 5px; background-color: #fff; min-height: 200px; }\n",
        "\"\"\", fill_height=True) as demo:\n",
        "    gr.HTML(\"<h1 style='color: #0056b3;'>üèõÔ∏è Law GPT Agent ‚öñÔ∏è</h1>\")\n",
        "    gr.Markdown(\"Your personal legal research assistant. Ask a question or upload a document.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        query_input = gr.Textbox(label=\"Enter your legal question:\", placeholder=\"e.g., What is the law on contracts in Pakistan?\")\n",
        "        file_input = gr.File(label=\"Upload relevant document (optional)\")\n",
        "\n",
        "    submit_button = gr.Button(\"Get Legal Insight\")\n",
        "\n",
        "    output_text = gr.Textbox(label=\"Legal Insight:\", interactive=False, lines=10, autoscroll=True)\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=law_gpt_interface,\n",
        "        inputs=[query_input, file_input],\n",
        "        outputs=output_text\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "Gtjg1R1ZI0fC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0J2VhmBVC4k3iD6H0NuUD"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}